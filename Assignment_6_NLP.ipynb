{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "# Check if MPS is available and set device accordingly\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('train-v1.1.json') as train_file:\n",
    "    train_data = json.load(train_file)\n",
    "\n",
    "with open('dev-v1.1.json') as dev_file:\n",
    "    dev_data = json.load(dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def format_qa_dataset(data):\n",
    "    context_list = []\n",
    "    question_list = []\n",
    "    answer_list = []\n",
    "\n",
    "    for article in data['data']:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context_text = paragraph['context']\n",
    "            for qa_pair in paragraph['qas']:\n",
    "                question_text = qa_pair['question']\n",
    "                answer_dict = qa_pair['answers'][0]  # Taking the first answer only\n",
    "                answer_dict['text'] = answer_dict['text']\n",
    "                answer_dict['answer_start'] = answer_dict['answer_start']\n",
    "\n",
    "                context_list.append(context_text)\n",
    "                question_list.append(question_text)\n",
    "                answer_list.append(answer_dict)\n",
    "    \n",
    "    return Dataset.from_dict({'context': context_list, 'question': question_list, 'answers': answer_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_formatted = format_qa_dataset(train_data)\n",
    "dev_dataset_formatted = format_qa_dataset(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "pretrained_model_name = 'distilbert/distilbert-base-uncased'\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align(examples):\n",
    "    clean_questions = [q.strip() for q in examples['question']]\n",
    "    tokenized_inputs = bert_tokenizer(\n",
    "        clean_questions,\n",
    "        examples['context'],\n",
    "        max_length=384,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    start_positions_list = []\n",
    "    end_positions_list = []\n",
    "    \n",
    "    for idx, ans in enumerate(examples['answers']):\n",
    "        start_positions_list.append(ans['answer_start'])\n",
    "        end_positions_list.append(ans['answer_start'] + len(ans['text']))\n",
    "    \n",
    "    tokenized_inputs.update({\n",
    "        \"start_positions\": start_positions_list,\n",
    "        \"end_positions\": end_positions_list,\n",
    "    })\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59f244a97b94d82a72d935dee418d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48435f7bcc04864b4e5ed2966830497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset_formatted.map(tokenize_and_align, batched=True)\n",
    "tokenized_dev_dataset = dev_dataset_formatted.map(tokenize_and_align, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(pretrained_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parameters = TrainingArguments(\n",
    "    output_dir=\"./qa_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = Trainer(\n",
    "    model=qa_model,\n",
    "    args=training_parameters,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_dev_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb06685e08747cb862ac1ab4acf4b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10950 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.9166, 'grad_norm': 3.2423887252807617, 'learning_rate': 1.9086757990867582e-05, 'epoch': 0.09}\n",
      "{'loss': 5.7097, 'grad_norm': 8.277706146240234, 'learning_rate': 1.8173515981735163e-05, 'epoch': 0.18}\n",
      "{'loss': 5.5541, 'grad_norm': 14.45805835723877, 'learning_rate': 1.726027397260274e-05, 'epoch': 0.27}\n",
      "{'loss': 5.4709, 'grad_norm': 8.233311653137207, 'learning_rate': 1.634703196347032e-05, 'epoch': 0.37}\n",
      "{'loss': 5.3793, 'grad_norm': 15.235696792602539, 'learning_rate': 1.54337899543379e-05, 'epoch': 0.46}\n",
      "{'loss': 5.3542, 'grad_norm': 16.16097068786621, 'learning_rate': 1.4520547945205482e-05, 'epoch': 0.55}\n",
      "{'loss': 5.3063, 'grad_norm': 19.32600212097168, 'learning_rate': 1.360730593607306e-05, 'epoch': 0.64}\n",
      "{'loss': 5.24, 'grad_norm': 18.49815559387207, 'learning_rate': 1.2694063926940641e-05, 'epoch': 0.73}\n",
      "{'loss': 5.2002, 'grad_norm': 22.82916259765625, 'learning_rate': 1.178082191780822e-05, 'epoch': 0.82}\n",
      "{'loss': 5.1659, 'grad_norm': 30.131479263305664, 'learning_rate': 1.08675799086758e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ffe766082c4d1cb3445a1c37d01c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.0377936363220215, 'eval_runtime': 177.0618, 'eval_samples_per_second': 59.697, 'eval_steps_per_second': 3.733, 'epoch': 1.0}\n",
      "{'loss': 5.1149, 'grad_norm': 22.424560546875, 'learning_rate': 9.95433789954338e-06, 'epoch': 1.0}\n",
      "{'loss': 4.9654, 'grad_norm': 21.65566062927246, 'learning_rate': 9.04109589041096e-06, 'epoch': 1.1}\n",
      "{'loss': 4.9639, 'grad_norm': 34.950931549072266, 'learning_rate': 8.127853881278539e-06, 'epoch': 1.19}\n",
      "{'loss': 4.9469, 'grad_norm': 15.26646614074707, 'learning_rate': 7.214611872146119e-06, 'epoch': 1.28}\n",
      "{'loss': 4.9264, 'grad_norm': 16.952180862426758, 'learning_rate': 6.301369863013699e-06, 'epoch': 1.37}\n",
      "{'loss': 4.9043, 'grad_norm': 21.248090744018555, 'learning_rate': 5.388127853881279e-06, 'epoch': 1.46}\n",
      "{'loss': 4.863, 'grad_norm': 23.045116424560547, 'learning_rate': 4.4748858447488585e-06, 'epoch': 1.55}\n",
      "{'loss': 4.8632, 'grad_norm': 15.300145149230957, 'learning_rate': 3.5616438356164386e-06, 'epoch': 1.64}\n",
      "{'loss': 4.8221, 'grad_norm': 16.76299476623535, 'learning_rate': 2.6484018264840183e-06, 'epoch': 1.74}\n",
      "{'loss': 4.8165, 'grad_norm': 20.57821273803711, 'learning_rate': 1.7351598173515982e-06, 'epoch': 1.83}\n",
      "{'loss': 4.8152, 'grad_norm': 21.72825813293457, 'learning_rate': 8.219178082191781e-07, 'epoch': 1.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3968a4b46bba4b61a123e44157ff6c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.827496528625488, 'eval_runtime': 176.7432, 'eval_samples_per_second': 59.804, 'eval_steps_per_second': 3.74, 'epoch': 2.0}\n",
      "{'train_runtime': 10116.0486, 'train_samples_per_second': 17.319, 'train_steps_per_second': 1.082, 'train_loss': 5.1433682933789955, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10950, training_loss=5.1433682933789955, metrics={'train_runtime': 10116.0486, 'train_samples_per_second': 17.319, 'train_steps_per_second': 1.082, 'total_flos': 1.7167621364554752e+16, 'train_loss': 5.1433682933789955, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_pipeline_model = pipeline(\"question-answering\", model=qa_model, tokenizer=bert_tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_context = \"I am Sumeru. I like playing Poker.\"\n",
    "sample_question = \"What does Sumeru like?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_result = qa_pipeline_model({\n",
    "    'context': sample_context,\n",
    "    'question': sample_question\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: {'score': 0.021213460713624954, 'start': 28, 'end': 33, 'answer': 'Poker'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction:\", qa_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_start_idx = qa_result['start']\n",
    "predicted_end_idx = qa_result['end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_answer = \"Poker\"\n",
    "actual_start_idx = sample_context.find(actual_answer)\n",
    "actual_end_idx = actual_start_idx + len(actual_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(pred_span, true_span):\n",
    "    pred_token_indices = set(range(pred_span['start_positions'], pred_span['end_positions']))\n",
    "    true_token_indices = set(range(true_span['start_positions'], true_span['end_positions']))\n",
    "    \n",
    "    if not pred_token_indices or not true_token_indices:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection_len = len(pred_token_indices & true_token_indices)\n",
    "    union_len = len(pred_token_indices | true_token_indices)\n",
    "    \n",
    "    return intersection_len / union_len if union_len != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_span = {'start_positions': predicted_start_idx, 'end_positions': predicted_end_idx}\n",
    "true_span = {'start_positions': actual_start_idx, 'end_positions': actual_end_idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: Poker\n",
      "True Answer: Poker\n",
      "Token-level IoU: 1.0\n"
     ]
    }
   ],
   "source": [
    "iou_score_value = calculate_iou(predicted_span, true_span)\n",
    "print(\"Predicted Answer:\", qa_result['answer'])\n",
    "print(\"True Answer:\", actual_answer)\n",
    "print(\"Token-level IoU:\", iou_score_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
